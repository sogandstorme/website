
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta property="og:title" content="ViPer: Visual Personalization of Generative Models via Individual Preference Learning"/>
  <meta property="og:url" content="https://clipasso.github.io/clipasso/"/>
  <meta property="og:image" content="static/images/og_tag_header_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <title>ViPer: Visual Personalization of Generative Models via Individual Preference Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="icon" href="static/figures/icon2.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script type="text/javascript" src="https://gc.kis.v2.scr.kaspersky-labs.com/FD126C42-EBFA-4E12-B309-BB3FDD723AC1/main.js?attr=9171MmH3WN39lpiFNBZkdAGt6QRMn6t9xAo7C_vxj6tZzYxb_z7uK9Jx1ZuU_w2Aps1WpTOmrQ7606P23I1A8g" charset="UTF-8"></script><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="publication-header">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- <div class="columns is-centered"> -->
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ViPer: Visual Personalization of Generative Models via Individual Preference Learning</h1>
        </div>
    </div>
  </div>   
</section>

<section class="publication-author-block">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title">ViPer: Visual Personalization of Generative Models via Individual Preference Learning</h1> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">Sogand Salehi,</span>
            <span class="author-block">Mahdi Shafiei,</span>
            <span class="author-block">Roman Bachmann,</span><br>
            <span class="author-block">Teresa Yeo,</span>
            <span class="author-block">Amir Zamir,</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block">Swiss Federal Institute of Technology (EPFL)</span> 
            <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2202.05822" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Colab Link. -->
              <span class="link-block">
                <a href="https://github.com/yael-vinker/CLIPSketch" target="_blank"
                class="external-link button is-normal is-rounded">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
              </span>
             <span class="link-block">
              <a href="https://colab.research.google.com/github/yael-vinker/CLIPasso/blob/main/CLIPasso.ipynb"  target="_blank"
                 class="external-link button is-normal is-rounded">
                <span class="icon">
                    <i class="fas fa-infinity"></i>
                </span>
                <span>Colab</span>
              </a>
             </span>
            
            <span class="link-block">
              <a href="https://replicate.com/yael-vinker/clipasso"  target="_blank"
                 class="external-link button is-normal is-rounded">
                <span class="icon">
                    <i class="fas fa-laptop"></i>
                </span>
                <span>Demo</span>
              </a>
             </span>
           
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <!-- <div class="hero-body"> -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
      <div class="container">
      <div class="item">
      <div class="column is-centered has-text-centered">
        <img src="static/images/newpf.png" alt="clipasso"/>
      </div>
      <h2 class="subtitle">
         We introduce ViPer, a visual personalization method, that captures individual preferences, generating results based on them without the need for long, detailed, and engineered prompts. Note how the results vary for different users when using the same prompt. 
      </h2>
    </div>
  </div>
 <!--  </div> -->
  </div>
  </div>
 <!--  </div> -->
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="item">
          <p style="margin-bottom: 30px">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/figures/sketchy_video.mp4"
          type="video/mp4">
        </video>
        </p>
        </div>
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Personalized image generation involves creating images aligned with an individual’s visual preference. 
            Current generative models are, however, tuned to produce outputs that appeal to a broad audience, and personalization to individual users' visual preferences relies on iterative and manual prompt engineering by the user, which is neither time-efficient nor scalable.
          </p><p>
            We propose to personalize the image generation process by first inviting users to comment on a small selection of images, explaining why they like or dislike each. 
            Based on these comments, we infer a user’s liked and disliked visual attributes, .ie, their visual preference, using a large language model. These attributes are used to guide a text-to-image model toward producing images that are personalized towards the individual user's visual preference.
            Through a series of user tests and large language model guided evaluations, we demonstrate that our proposed method results in generations that are well aligned with individual users' visual preferences.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">More Visualization</h2>
      <div id="results-carousel" class="carousel results-carousel">
      <div class="column is-centered has-text-centered">
        <img src="static/images/fig0.png" alt="cars peace"/>
      </div>
      <div class="column is-centered has-text-centered">
        <img src="static/images/fig1.png" alt="cars peace"/>
      </div>
      <div class="column is-centered has-text-centered">
        <img src="static/images/fig2.png" alt="cars peace"/>
      </div>
      <div class="column is-centered has-text-centered">
        <img src="static/images/fig3.png" alt="cars peace"/>
      </div>
      <div class="column is-centered has-text-centered">
        <img src="static/images/fig4.png" alt="cars peace"/>
      </div>
      <div class="column is-centered has-text-centered">
        <img src="static/images/fig5.png" alt="cars peace"/>
      </div>
      
  </div>
</div>
</div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">How does it work?</h2>
        <div class="content has-text-justified">
          <p>

            Our method is optimization-based and therefore does not require any explicit sketch dataset. 
            We use the CLIP image encoder to guide the process of converting a photograph to an abstract sketch.

            CLIP encoding provides the semantic understanding of the concept depicted, while the photograph itself provides the geometric grounding of the sketch to the concrete subject.

            We define a sketch as a set of N black strokes placed on a white background.
            We vary the number of strokes N to create different levels of abstraction. 
          </p>
        </div>
        <div class="column is-centered has-text-centered">
        <img src="static/images/overview_figure.png" alt="cars peace"/>
        </div>
        <div class="content has-text-justified">
          <p>
            Given a target image I of the desired subject, our goal is to synthesize the corresponding sketch S while maintaining both the semantic and geometric attributes of the subject.
            We begin by extracting the salient regions of the input image to define the initial locations of the strokes.
            Next, in each step of the optimization we feed the stroke parameters to a differentiable rasterizer to produce the rasterized sketch. The resulting sketch, as well as the original image are then fed into CLIP to define a CLIP-based perceptual loss. 
            The key to the success of our method is to use the intermediate layers of a pretrained CLIP model to constrain the geometry of the output sketch. Without this term, the output sketch would not be similar to the input image.
            We back-propagate the loss through the differentiable rasterizer and update the strokes' control points directly at each step until convergence of the loss function.
            The learned parameters and loss terms are highlighted in red, while the blue components are frozen during the entire optimization process, solid arrows are used to mark the backpropagation path.
          </p>
        <div class="column is-centered has-text-centered">
        <img src="static/images/IDEFICS.png" alt="cars peace"/>
        </div>
          <p>
            p of the optimization we feed the stroke parameters to a differentiable rasterizer to produce the rasterized sketch. The resulting sketch, as well as the original image are then fed into CLIP to define a CLIP-based perceptual loss. 
            The key to the success of our method is to use the intermediate layers of a pretrained CLIP model to constrain the geometry of the output sketch. Without this term, the output sketch would not be similar to the input image.
            We back-propagate the loss through the differentiable
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>


</section>







<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper poster -->

      <h2 class="title has-text-centered">Personalized Images for 10 Agents</h2>
      <p style="margin-top: 30px; margin-bottom: 30px">
      <!-- <div class="column is-four-fifths"> -->
          <div class="column is-centered has-text-centered">
        <img src="static/images/10_10fig.png" alt="cars peace"/>
      </div>
       </p>
        <!--/ Paper poster -->
      </div>
    </div>

  </section>






  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      TODO
    </div>
</section>



<footer class="footer">
 <!--  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
      href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
      <i class="fas fa-file-pdf"></i>
    </a>
    <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
      <i class="fab fa-github"></i>
    </a>
  </div> -->
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>


  <script type="text/javascript">
    var sc_project=12351448; 
    var sc_invisible=1; 
    var sc_security="c676de4f"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
    href="https://statcounter.com/" target="_blank"><img
    class="statcounter"
    src="https://c.statcounter.com/12351448/0/c676de4f/1/"
    alt="Web Analytics"></a></div></noscript>
    <!-- End of Statcounter Code -->

  </body>
  </html>
